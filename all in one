#--------------------------Apriori---------------------------------------------------#
data = [['Milk','Onion','Nutmeg','Kidney beans','Eggs','Yogurt'],
 ['Dill','Onion','Nutmeg','Kidney beans','Eggs','Yogurt'],
 ['Milk','Apple','Kidney beans','Eggs'],
 ['Milk','Unicorn','Corn','Kidney beans','Yogurt'],
 ['Corn','Onion','Onion','Kidney beans','Ice cream','Eggs']]
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
print(data)
te = TransactionEncoder()
te_ary = te.fit(data).transform(data)
df = pd.DataFrame(te_ary, columns=te.columns_)
print(df)

from mlxtend.frequent_patterns import apriori
frequent_itemsets = apriori(df,min_support=0.5, use_colnames=True)
print(frequent_itemsets)
from mlxtend.frequent_patterns import association_rules
res = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.6)
print(res)
res1 = res[['antecedents','consequents','support','confidence','lift']]
print(res1)
res2 = res1[res1['confidence'] >=1]
print(res2)

#----------------------------------------Decision tree-----------------------------------------------#

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
data=pd.read_csv("weather.csv")
print(data.columns )
from sklearn.preprocessing import LabelEncoder
data['outlook']=LabelEncoder().fit_transform(data['outlook'])
data['temperature']=LabelEncoder().fit_transform(data['temperature'])
data['humidity']=LabelEncoder().fit_transform(data['humidity'])
data['windy']=LabelEncoder().fit_transform(data['windy'])
data['play']=LabelEncoder().fit_transform(data['play'])
print(data)
# Lets split the training data and its coresponding prediction values.
# y - holds all the decisions.
# X - holds the training data.
y= data['play']
x = data.drop(['play'],axis=1)
# Fitting the model
from sklearn import tree
dt=tree.DecisionTreeClassifier(criterion="entropy")
dt=dt.fit(x,y)
tree.plot_tree(dt)
plt.show()

#------------------------------------------Naive Bayes--------------------------------------#


import pandas as pd
import numpy as np
data=pd.read_csv("iris.csv")
x=data.iloc[:,:-1].values
print(x)
y=data.iloc[:,-1].values
print(y)
#Encoding the category
from sklearn.preprocessing import LabelEncoder
y=LabelEncoder().fit_transform(y)
print(y)
#split train and test data
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=3)
print(x_train)
print(y_train)
#trian the model
from sklearn.naive_bayes import GaussianNB
gnb=GaussianNB()
gnb.fit(x_train,y_train)
#Prediction using Baye
y_predict=gnb.predict(x_test)
print("Predicted Value by model ",y_predict)
print("Actual value in dataset ",y_test)
#confusion matrix
from sklearn.metrics import confusion_matrix
confusion_mat=confusion_matrix(y_test,y_predict)
print("confusion matrix \n",confusion_mat)
#Classifier Accuracy
from sklearn.metrics import accuracy_score
accuracy=accuracy_score(y_test,y_predict)*100
print("Accuracy of Gaussian Model",accuracy)
'''input=[5.1, 3.5 ,1.4, 0.2]
output=gnb.predict(input)
print("predicted output\n",output)'''

#----------------------------------------------Regression--------------------------------#
#Assignment based on simple linear regression on any dataset
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
data=pd.read_csv("houseprices.csv")
print(data)
x=data.iloc[0:100,1:2].values
print(x)
y=data.iloc[0:100,:1].values
print(y)
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=3)
print(x_train)
print(len(x_train))
from sklearn.linear_model import LinearRegression
reg=LinearRegression()
reg.fit(x_train,y_train)
predicted=reg.predict(x_test)
print("intercept \n",reg.intercept_)
print("slope or weight \n",reg.coef_)
from sklearn.metrics import mean_squared_error
print("mean squared error is: \n",mean_squared_error(y_test,predicted))
print("predicted data by model \n",predicted)
print("Actual data \n",y_test)
plt.scatter(x_test,y_test)
plt.xlabel("area")
plt.ylabel("price")
plt.title("houseprices using linear regression \n")
plt.plot(x_test,predicted)
plt.show()
print("price: \n",reg.predict([[1802]]))

#-------------------------------------------Kmeans--------------------------------------------#
# importing libraries
import numpy as nm
import matplotlib.pyplot as mtp
import pandas as pd
data=pd.read_csv("Mall_Customers.csv")
print(data)
x=data.iloc[:,[3,4]].values
print(x)
#Training the K-means algorithm on the training dataset
from sklearn.cluster import KMeans
k=KMeans(n_clusters=5, init='k-means++', random_state= 42)
clusters=k.fit_predict(x)
print("number of clusters are \n",clusters)
#Visualizing the Clusters
mtp.scatter(x[clusters==0,0],x[clusters==0,1],s=100,c='blue',label='Cluster 1')
mtp.scatter(x[clusters==1,0],x[clusters==1,1],s=100,c='green',label='Cluster 2')
mtp.scatter(x[clusters==2,0],x[clusters==2,1],s=100,c='red',label='Cluster 3')
mtp.scatter(x[clusters==3,0],x[clusters==3,1],s=100,c='cyan',label='Cluster 4')
mtp.scatter(x[clusters==4,0],x[clusters==4,1],s=100,c='magenta',label='Cluster 5')
mtp.scatter(k.cluster_centers_[:, 0], k.cluster_centers_[:, 1], s = 300, c =
'yellow', label = 'Centroid')
mtp.title('Clusters of customers')
mtp.xlabel('Annual Income (k$)')
mtp.ylabel('Spending Score (1-100)')
mtp.legend()
mtp.show()

#--------------------------------------Deep Learning -----------------------------------#
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from matplotlib import  pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

data = pd.read_csv("C:/Users/Hp/Desktop/Practicles/ML/pract8/diabetes2.csv")
print(data.head(5))

import seaborn as sns
print(data['Outcome'].value_counts().plot(kind = 'bar'))

predictors = data.iloc[:,0:8]
response = data.iloc[:,8]

x_train,x_test,y_train,y_test = train_test_split(predictors,response,test_size=0.2)
print(x_train.shape,y_train.shape)
print(x_train.shape,y_test.shape)

kerasmodel = Sequential()
kerasmodel.add(Dense(12,input_dim=8,activation='relu'))
kerasmodel.add(Dense(8,activation='relu'))
kerasmodel.add(Dense(1,activation='sigmoid'))

kerasmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

model=kerasmodel.fit(x_train,y_train,epochs=300,batch_size=15)

_, accuracy = kerasmodel.evaluate(x_train,y_train)
print('Train Accuracy: %2f' % (accuracy*100))

from sklearn.metrics import  accuracy_score
y_pred = kerasmodel.predict(x_test)
y_pred = np.round(y_pred).astype(int)

accuracy_score(y_test,y_pred)

from ann_visualizer.visualize import ann_viz

ann_viz(kerasmodel, title="Diabetes prediction")

#Use Datasets
# For Decision Tree use weather.csv
# For Naive Bayes use Iris.csv
# For Regression Houseprice.csv
# For KMeans Mall_Price.csv
# For Deep learning diabetes2.csv
Footer
